{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ElCald/CIFAR10/blob/main/1_comparaison_ts.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Premier Exemple\n"
      ],
      "metadata": {
        "id": "dVfqIeiqYIQv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cet exemple crée une série temporelle et compare la performance de différentes techniques :\n",
        "* une méthode persistante (Dummy) qui ne fait que recopier la dernière valeur\n",
        "  * c'est notre **baseline**\n",
        "* une interpolation des valeurs\n",
        "* une méthode non-IA très efficace (Holt-Winter)\n",
        "* un simple réseau de neurones fully-connected"
      ],
      "metadata": {
        "id": "jbJPekTsPF64"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3WWenzEO1Luf"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import random\n",
        "from math import sin, cos\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "\n",
        "random.seed(1)\n",
        "torch.manual_seed(1)\n",
        "\n",
        "features = 256\n",
        "ts_len = 3_000\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Génération de la série temporelle\n",
        "\n",
        "Pour cette génération, on fera une séquence **random walk**, où chaque entrée est le résultat d'une variation contrôlée. Même si ce n'est pas explicite, les entrées sont séquentiellement dépendantes à cause de *sin* et *cos*"
      ],
      "metadata": {
        "id": "ge_65RkKPFO3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_time_series_data(length):\n",
        "    a = .2\n",
        "    b = 300\n",
        "    c = 20\n",
        "    ls = 5\n",
        "    ms = 20\n",
        "    gs = 100\n",
        "\n",
        "    ts = []\n",
        "\n",
        "    for i in range(length):\n",
        "        ts.append(b + a * i + ls * sin(i / 5) + ms * cos(i / 24) + gs * sin(i / 120) + c * random.random())\n",
        "\n",
        "    return ts\n"
      ],
      "metadata": {
        "id": "36muLxCy2GDF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mise en forme du dataset\n",
        "\n",
        "La fonction précédente génère une liste de valeurs. La fonction suivante met en forme les données sous forme de *sliding window*. Chaque entrée sera composée de :\n",
        "* X - les nombre de valeurs \"précédentes\" (`features`)\n",
        "* Y - le nombre de sorties (ici, juste **1**)\n",
        "\n",
        "Ensuite, le tableau avec les différentes entrées est découpé en train, val et test."
      ],
      "metadata": {
        "id": "zYjFW7-RPChB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_time_series_datasets(features, ts_len):\n",
        "    ts = get_time_series_data(ts_len)\n",
        "\n",
        "    X = []\n",
        "    Y = []\n",
        "    for i in range(features + 1, ts_len):\n",
        "        X.append(ts[i - (features + 1):i - 1])\n",
        "        Y.append([ts[i]])\n",
        "\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.3, shuffle = False)\n",
        "    X_val, X_test, Y_val, Y_test = train_test_split(X_test, Y_test, test_size = 0.5, shuffle = False)\n",
        "\n",
        "    x_train = torch.tensor(data = X_train)\n",
        "    y_train = torch.tensor(data = Y_train)\n",
        "\n",
        "    x_val = torch.tensor(data = X_val)\n",
        "    y_val = torch.tensor(data = Y_val)\n",
        "\n",
        "    x_test = torch.tensor(data = X_test)\n",
        "    y_test = torch.tensor(data = Y_test)\n",
        "\n",
        "    return x_train, x_val, x_test, y_train, y_val, y_test\n",
        "\n",
        "\n",
        "x_train, x_val, x_test, y_train, y_val, y_test = get_time_series_datasets(features,ts_len)"
      ],
      "metadata": {
        "id": "DV8hKGEC2F5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercice :\n",
        "\n",
        "La fonction précédente n'utilise pas les classes Dataset et Dataloader de Pytorch. **Proposer une implémentation avec Datasest et Dataloader**."
      ],
      "metadata": {
        "id": "QqQ28qfbR9rV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# votre code ICI\n",
        "class CustomDataset(Dataset):\n",
        "    \"\"\"Dataset pour séries temporelles (X, Y)\"\"\"\n",
        "    def __init__(self, X, Y):\n",
        "        self.X = X\n",
        "        self.Y = Y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.Y[idx]\n",
        "\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "train_loader = DataLoader(CustomDataset(x_train, y_train), batch_size=batch_size, shuffle=True)\n",
        "val_loader   = DataLoader(CustomDataset(x_val, y_val), batch_size=batch_size, shuffle=False)\n",
        "test_loader  = DataLoader(CustomDataset(x_test, y_test), batch_size=batch_size, shuffle=False)\n"
      ],
      "metadata": {
        "id": "9Jchj5MuSTIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### La classe DummyPredictor\n",
        "\n",
        "Dans une série temporelle, le *baseline* est souvent une implémentation du type `persistence`. Cela veut dire qu'aucune prédiction n'est faite, mais on finit juste par recopier la dernière sortie.\n",
        "\n",
        "Ça semble illogique mais c'est un bon test de votre modèle : souvent, un modèle persistant obtient des excellents résultats avec la métrique loss (ex : *mse*) car dan la plupart des cas il n'est pas loin du résultat réel.\n",
        "\n",
        "Un bon modèle doit être plus performant que le DummyPredictor."
      ],
      "metadata": {
        "id": "G0l6F7I_SVDk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DummyPredictor(torch.nn.Module):\n",
        "\n",
        "    def forward(self, x):\n",
        "        last_values = []\n",
        "        for r in x.tolist():\n",
        "            last_values.append([r[-1]])\n",
        "        return torch.tensor(data = last_values)"
      ],
      "metadata": {
        "id": "kMU6lqEU2Xuo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### La classe Interpolation\n",
        "\n",
        "Autre forme \"simple\" est l'interpolation des valeurs. On prend les `features` et essaye de prédire la prochaine valeur (d'où l'option *extrapolate*)."
      ],
      "metadata": {
        "id": "1kl-E3WdTJUH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import interpolate\n",
        "\n",
        "class InterpolationPredictor(torch.nn.Module):\n",
        "\n",
        "    def forward(self, x):\n",
        "        last_values = []\n",
        "        values = x.tolist()\n",
        "        for v in values:\n",
        "            x = np.arange(0, len(v))\n",
        "            y = interpolate.interp1d(x, v, fill_value = 'extrapolate')\n",
        "            last_values.append([y(len(v)).tolist()])\n",
        "        return torch.tensor(data = last_values)\n"
      ],
      "metadata": {
        "id": "SqfdMidT2Ylk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### La classe Holt-Winter\n",
        "Enfin, on crée une classe qui utilise la bibliothèque `statsmodels`. C'est une bibliothèque très intéressante, avec plusieurs méthodes statistiques très efficaces pour la régression linéaire (dont ANOVA) et les séries temporelles. Je vous conseille de regarder sa [documentation](https://www.statsmodels.org/stable/user-guide.html)."
      ],
      "metadata": {
        "id": "I0_MH5pqTiS_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
        "\n",
        "class HwesPredictor(torch.nn.Module):\n",
        "\n",
        "    def forward(self, x):\n",
        "        last_values = []\n",
        "        for r in x.tolist():\n",
        "            model = ExponentialSmoothing(r)\n",
        "            results = model.fit()\n",
        "            forecast = results.forecast()\n",
        "            last_values.append([forecast[0]])\n",
        "        return torch.tensor(data = last_values)\n"
      ],
      "metadata": {
        "id": "YUjPKF7H2z7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Le challenger : un réseau de neurones\n",
        "Pour finir, un simple réseau de neurones fully connected."
      ],
      "metadata": {
        "id": "4i3gnUZjU2T6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class FCNN(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, n_inp, l_1, l_2, n_out):\n",
        "        super(FCNN, self).__init__()\n",
        "        self.lin1 = torch.nn.Linear(n_inp, l_1)\n",
        "        self.lin2 = torch.nn.Linear(l_1, l_2)\n",
        "        self.lin3 = torch.nn.Linear(l_2, n_out)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = F.relu(self.lin1(x))\n",
        "        x2 = F.relu(self.lin2(x1))\n",
        "        y = self.lin3(x2)\n",
        "        return y"
      ],
      "metadata": {
        "id": "VAVgbI-m20wq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## L'heure de la vérite !"
      ],
      "metadata": {
        "id": "kDpVTKtLVdC0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#instantiation des modèles\n",
        "\n",
        "dummy_predictor = DummyPredictor()\n",
        "interpolation_predictor = InterpolationPredictor()\n",
        "hwes_predictor = HwesPredictor()\n",
        "\n",
        "net = FCNN(n_inp = features, l_1 = 64, l_2 = 32, n_out = 1)\n",
        "\n"
      ],
      "metadata": {
        "id": "hVl2qT7S3FSd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Entraîner le réseau de neurones\n",
        "Le FCNN nécessite un entraînement, les autres s'appliquent directement sur les données"
      ],
      "metadata": {
        "id": "ceZD1puUVtWT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "net.train()\n",
        "\n",
        "optimizer = torch.optim.Adam(params = net.parameters())\n",
        "loss_func = torch.nn.MSELoss()\n",
        "\n",
        "best_model = None\n",
        "min_val_loss = 1_000_000\n",
        "\n",
        "training_loss = []\n",
        "validation_loss = []\n",
        "\n",
        "\"\"\"\n",
        "for t in range(10_000):\n",
        "\n",
        "    prediction = net(x_train)\n",
        "    loss = loss_func(prediction, y_train)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    val_prediction = net(x_val)\n",
        "    val_loss = loss_func(val_prediction, y_val)\n",
        "\n",
        "    training_loss.append(loss.item())\n",
        "    validation_loss.append(val_loss.item())\n",
        "\n",
        "    if val_loss.item() < min_val_loss:\n",
        "        best_model = copy.deepcopy(net)\n",
        "        min_val_loss = val_loss.item()\n",
        "        #print(t,\" - meilleur\")\n",
        "\n",
        "    if t % 1000 == 0:\n",
        "        print(f'epoch {t}: train - {round(loss.item(), 4)}, val: - {round(val_loss.item(), 4)}')\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "for epoch in range(10_000):\n",
        "\n",
        "        # --- Entraînement sur mini-batchs ---\n",
        "    net.train()\n",
        "    running_loss = 0.0\n",
        "    for xb, yb in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        pred = net(xb)\n",
        "        loss = loss_func(pred, yb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * xb.size(0)\n",
        "    train_loss_epoch = running_loss / len(train_loader.dataset)\n",
        "\n",
        "    # --- Validation ---\n",
        "    net.eval()\n",
        "    with torch.no_grad():\n",
        "        val_running_loss = 0.0\n",
        "        for xb, yb in val_loader:\n",
        "            val_pred = net(xb)\n",
        "            val_loss = loss_func(val_pred, yb)\n",
        "            val_running_loss += val_loss.item() * xb.size(0)\n",
        "        val_loss_epoch = val_running_loss / len(val_loader.dataset)\n",
        "\n",
        "    training_loss.append(train_loss_epoch)\n",
        "    validation_loss.append(val_loss_epoch)\n",
        "\n",
        "    # --- Sauvegarde du meilleur modèle ---\n",
        "    if val_loss_epoch < min_val_loss:\n",
        "        best_model = copy.deepcopy(net)\n",
        "        min_val_loss = val_loss_epoch\n",
        "\n",
        "    # --- Affichage périodique ---\n",
        "    if epoch % 1000 == 0:\n",
        "        print(f\"epoch {epoch}: train - {train_loss_epoch:.4f}, val - {val_loss_epoch:.4f}\")\n",
        "\n",
        "net.eval()\n",
        "\n"
      ],
      "metadata": {
        "id": "Bcb_iSwIVbDd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9ee98f3-e9bf-4b6c-d887-318245b7a066"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0: train - 38968.3988, val - 19096.8290\n",
            "epoch 1000: train - 45.6999, val - 81.5005\n",
            "epoch 2000: train - 45.4965, val - 38.3323\n",
            "epoch 3000: train - 40.4447, val - 38.5280\n",
            "epoch 4000: train - 39.2301, val - 41.3126\n",
            "epoch 5000: train - 34.1255, val - 37.9303\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.title(\"Training progress\")\n",
        "plt.yscale(\"log\")\n",
        "plt.plot(training_loss, label = 'training loss')\n",
        "plt.plot(validation_loss, label = 'validation loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "qMoQ4Wrm3YK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparer les loss\n",
        "Ceci donne déjà une idée de l'erreur moyenne de chaque modèle"
      ],
      "metadata": {
        "id": "9RMsbLz8V7Sm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Testing')\n",
        "print(f'FCNN Loss: {loss_func(best_model(x_test), y_test).item()}')\n",
        "print(f'Dummy Loss: {loss_func(dummy_predictor(x_test), y_test).item()}')\n",
        "print(f'Linear Interpolation Loss: {loss_func(interpolation_predictor(x_test), y_test).item()}')\n",
        "print(f'HWES Loss: {loss_func(hwes_predictor(x_test), y_test).item()}')"
      ],
      "metadata": {
        "id": "rlBwqr9W5RS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sans surprise, HWES et FCNN ont les erreurs les plus faibles.\n",
        "Il est fort probable que le modèle *dummy* se porte bien également, par rapport à l'interpolation."
      ],
      "metadata": {
        "id": "lcrxckJ9WPRq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Affichage des prédictions FCNN et HWES"
      ],
      "metadata": {
        "id": "2FcN5yCvWMN2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.title('Test')\n",
        "plt.plot(y_test, '--', label = 'actual')\n",
        "plt.plot(best_model(x_test).tolist(), label = 'FCNN')\n",
        "plt.plot(hwes_predictor(x_test).tolist(), label = 'HWES')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "I2cxSISP4zFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "avec un petit zoom"
      ],
      "metadata": {
        "id": "JwPTNEzzWjMU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.title('Test - Zoom')\n",
        "plt.plot(y_test[50:250], '--', label = 'actual')\n",
        "plt.plot(best_model(x_test).tolist()[50:250], label = 'FCNN')\n",
        "plt.plot(hwes_predictor(x_test).tolist()[50:250], label = 'HWES')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "oE2qXmHz3ga1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analyse des prédictions : qui gagne ?\n",
        "\n",
        "Dans ce graphique, on affiche l'erreur de FCN et de HWES pour chaque \"entrée\" du dataset de test.\n",
        "\n",
        "Cepenant, on calcule qui a l'erreur (absolute) la plus petite. La fonction `ReLU` \"bloque\" ceux qui ont l'erreur la plus grande.\n",
        "\n",
        "Le résultat est qu'on met en **vert** les entrées pour lesquelles le FCNN présente la plus petite erreur, et en **rouge** celles du HWES.\n",
        "\n",
        "Si vous avez entrainé le FCNN jusqu'à la fin, vous devez voir un peu plus de lignes vertes que rouges.\n",
        "Si les lignes vertes sont \"plus hautes\" que les rouges, cela veut dire que les HWES avait une erreur encore plus élevée pour ces entrées. On peut donc imaginer que vert est juste \"derrière la course\" les fois où rouge gagne."
      ],
      "metadata": {
        "id": "GzPqFQerWlw_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_n = len(y_test)\n",
        "net_abs_dev = (best_model(x_test) - y_test).abs_()\n",
        "hwes_abs_dev = (hwes_predictor(x_test) - y_test).abs_()\n",
        "diff_pos = F.relu(hwes_abs_dev - net_abs_dev).reshape(test_n).tolist()\n",
        "diff_min = (-F.relu(net_abs_dev - hwes_abs_dev)).reshape(test_n).tolist()\n",
        "plt.title('HWES Predictor VS FCNN Predictor')\n",
        "plt.hlines(0, xmin = 0, xmax = test_n, linestyles = 'dashed')\n",
        "plt.bar(list(range(test_n)), diff_pos, color = 'g', label = 'FCNN Wins')\n",
        "plt.bar(list(range(test_n)), diff_min, color = 'r', label = 'HWES Wins')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "M1EazmAS3gPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercice 2\n",
        "Modifier le code précédent pour compter combien de fois chaque stratégie gagne."
      ],
      "metadata": {
        "id": "gqrmjwUhZDOD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercice 3\n",
        "\n",
        "Dans le code précédent, nous n'avons pas utilisé vos dataloaders...\n",
        "C'est l'heure de les modifier et faire un code pytorch \"propre\".Rendre votre code sur Moodle"
      ],
      "metadata": {
        "id": "lDI98xbiX02-"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hdOc_x2TUkm4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}